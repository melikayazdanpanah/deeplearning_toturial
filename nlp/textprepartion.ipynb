{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Keras and TensorFlow: Ensure you have the necessary libraries installed.\n",
    "Import Libraries: Use Tokenizer and pad_sequences from Keras.\n",
    "Prepare Data: Define your text data.\n",
    "Initialize Tokenizer: Create a Tokenizer and fit it to your text.\n",
    "Convert to Sequences: Transform text into sequences of integers.\n",
    "Reverse Transformation: Optionally, convert sequences back to text.\n",
    "Pad Sequences: Ensure all sequences have the same length for model compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some sample lines of text that you want to process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\n",
    " 'The quick brown fox',\n",
    " 'Jumps over $$$ the lazy brown dog',\n",
    " 'Who jumps high into the blue sky after counting 123',\n",
    " 'And quickly returns to earth'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Tokenizer object and fit it on your text data. This step builds a dictionary of all unique words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the texts_to_sequences method to convert your text lines into sequences of integers. Each integer represents a unique word from the tokenizerâ€™s dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "print(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = tokenizer.sequences_to_texts(sequences)\n",
    "print(texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For use in deep learning models, sequences often need to be of the same length. Use pad_sequences to ensure that all sequences are of equal length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "print(padded_sequences)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
