{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Keras and TensorFlow: Ensure you have the necessary libraries installed.\n",
    "Import Libraries: Use Tokenizer and pad_sequences from Keras.\n",
    "Prepare Data: Define your text data.\n",
    "Initialize Tokenizer: Create a Tokenizer and fit it to your text.\n",
    "Convert to Sequences: Transform text into sequences of integers.\n",
    "Reverse Transformation: Optionally, convert sequences back to text.\n",
    "Pad Sequences: Ensure all sequences have the same length for model compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 21:57:17.633138: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-06 21:57:17.633677: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-06 21:57:17.636620: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-06 21:57:17.643230: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-06 21:57:17.653903: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-06 21:57:17.657228: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-06 21:57:17.668001: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-06 21:57:18.299429: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some sample lines of text that you want to process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\n",
    " 'The quick brown fox',\n",
    " 'Jumps over $$$ the lazy brown dog',\n",
    " 'Who jumps high into the blue sky after counting 123',\n",
    " 'And quickly returns to earth'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Tokenizer object and fit it on your text data. This step builds a dictionary of all unique words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the texts_to_sequences method to convert your text lines into sequences of integers. Each integer represents a unique word from the tokenizer’s dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 2, 5], [3, 6, 1, 7, 2, 8], [9, 3, 10, 11, 1, 12, 13, 14, 15, 16], [17, 18, 19, 20, 21]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "print(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the quick brown fox', 'jumps over the lazy brown dog', 'who jumps high into the blue sky after counting 123', 'and quickly returns to earth']\n"
     ]
    }
   ],
   "source": [
    "texts = tokenizer.sequences_to_texts(sequences)\n",
    "print(texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For use in deep learning models, sequences often need to be of the same length. Use pad_sequences to ensure that all sequences are of equal length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  4  2  5  0  0  0  0  0  0]\n",
      " [ 3  6  1  7  2  8  0  0  0  0]\n",
      " [ 9  3 10 11  1 12 13 14 15 16]\n",
      " [17 18 19 20 21  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "print(padded_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converts text to lowercase and\n",
    "removes symbols,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/melika_yazdanpanah/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/melika_yazdanpanah/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 1, 4], [2, 5, 1, 6], [2, 7, 8, 9, 10], [11, 12, 13]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = [\n",
    " 'The quick brown fox',\n",
    " 'Jumps over $$$ the lazy brown dog',\n",
    " 'Who jumps high into the blue sky after counting 123',\n",
    " 'And quickly returns to earth'\n",
    "]\n",
    "def remove_stop_words(text):\n",
    "   text = word_tokenize(text.lower())\n",
    "   stop_words = set(stopwords.words('english'))\n",
    "   text = [word for word in text if word.isalpha() and not word in stop_words]\n",
    "   return ' '.join(text)\n",
    "lines = list(map(remove_stop_words, lines))\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "tokenizer.texts_to_sequences(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  4  2  5]\n",
      " [ 1  7  2  8]\n",
      " [13 14 15 16]\n",
      " [18 19 20 21]]\n"
     ]
    }
   ],
   "source": [
    "padded_sequences = pad_sequences(sequences, maxlen=4)\n",
    "print(padded_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stop words frequently has little or no effect on text clas‐\n",
    "sification tasks. If you simply want to remove numbers from text\n",
    "input to a neural network without removing stop words, create a\n",
    "Tokenizer this way:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\n",
    " filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_` ' \\\n",
    " '{|}~\\t\\n0123456789')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s important that text input to a neural network for predictions be tokenized and\n",
    "padded in the same way as text input to the model for training. If you’re thinking\n",
    "it sure would be nice not to have to do the tokenization and sequencing manually,\n",
    "there is a way around it. Rather than write a lot of code, you can include a\n",
    "TextVectorization layer in the model. I’ll demonstrate how momentarily. But first,\n",
    "you need to learn about embedding layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
